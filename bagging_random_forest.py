#-------------------------------------------------------------------------
# AUTHOR: Jonathan Lu
# FILENAME: bagging_random_forest.py
# SPECIFICATION: Implement ensemble method and use random forest
# FOR: CS 4210 - Assignment #3
# TIME SPENT: 4 hours
#-----------------------------------------------------------*/

#IMPORTANT NOTE: DO NOT USE ANY ADVANCED PYTHON LIBRARY TO COMPLETE THIS CODE SUCH AS numpy OR pandas. You have to work here only with standard vectors and arrays
#importing some Python libraries
from sklearn import tree
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
import csv

dbTraining = []
dbTest = []
X_training = []
Y_training = []
classVotes = [] #this array will be used to count the votes of each classifier

#reading the training data in a csv file
with open('C:/Users/Administrator/Desktop/optdigits.tra', 'r') as trainingFile:
    reader = csv.reader(trainingFile)
    for i, row in enumerate(reader):
        dbTraining.append (row)

#reading the test data in a csv file
with open('C:/Users/Administrator/Desktop/optdigits.tes', 'r') as testingFile:
    reader = csv.reader(testingFile)
    for i, row in enumerate(reader):
        dbTest.append (row)
        classVotes.append([0,0,0,0,0,0,0,0,0,0]) #inititalizing the class votes for each test sample

    print("Started my base and ensemble classifier ...")

    for k in range(20): #we will create 20 bootstrap samples here (k = 20). One classifier will be created for each bootstrap sample

        bootstrapSample = resample(dbTraining, n_samples=len(dbTraining), replace=True)
        trueCount = 0

        #populate the values of X_training and Y_training by using the bootstrapSample
        #--> add your Python code here
        for instance in bootstrapSample:
            X_training.append(instance[:-1]) #populates X training
            Y_training.append(instance[-1]) #populates Y training

        #fitting the decision tree to the data
        clf = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=None) #we will use a single decision tree without pruning it
        clf = clf.fit(X_training, Y_training)
       
        for i, testSample in enumerate(dbTest):
            #make the classifier prediction for each test sample and update the corresponding index value in classVotes.
            class_predicted = int(clf.predict([testSample[:-1]])[0])
            classVotes[i][class_predicted] += 1
            
            if k == 0: #for only the first base classifier, compare the prediction with the true label of the test sample here to start calculating its accuracy
                #--> add your Python code here
                if class_predicted == int(testSample[-1]):
                    trueCount += 1

        if k == 0: #for only the first base classifier, print its accuracy here
            #--> add your Python code here
            accuracy = trueCount / len(dbTest)
            print("Finished my base classifier (fast but relatively low accuracy) ...")
            print("My base classifier accuracy: " + str(accuracy))
            print("")

    #now, compare the final ensemble prediction (majority vote in classVotes) for each test sample with the ground truth label to calculate the accuracy of the ensemble classifier (all base classifiers together)
    trueCount = 0
    for i in range(len(dbTest)):
        if int(dbTest[i][-1]) == classVotes[i].index(max(classVotes[i])): # compare the true class in the test data to the most rating in classVotes
            trueCount += 1
    accuracy = trueCount / len(dbTest)

    #printing the ensemble accuracy here
    print("Finished my ensemble classifier (slow but higher accuracy) ...")
    print("My ensemble accuracy: " + str(accuracy))
    print("")

    print("Started Random Forest algorithm ...")

    #Create a Random Forest Classifier
    clf=RandomForestClassifier(n_estimators=20) #this is the number of decision trees that will be generated by Random Forest. The sample of the ensemble method used before

    #Fit Random Forest to the training data
    clf.fit(X_training,Y_training)

    #make the Random Forest prediction for each test sample. Example: class_predicted_rf = clf.predict([[3, 1, 2, 1, ...]]
    #--> add your Python code here
    trueCount = 0
    for testSample in dbTest:
        class_predicted_rf = int(clf.predict([testSample[:-1]])[0])
        #compare the Random Forest prediction for each test sample with the ground truth label to calculate its accuracy
        #--> add your Python code here
        if class_predicted_rf == int(testSample[-1]):
            trueCount += 1
    accuracy = trueCount / len(dbTest)

    #printing Random Forest accuracy here
    print("Finished Random Forest algorithm (much faster and higher accuracy!) ...")
    print("Random Forest accuracy: " + str(accuracy))